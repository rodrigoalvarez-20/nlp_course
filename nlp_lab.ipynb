{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup as bs4\n",
    "import nltk\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import spacy\n",
    "import json\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import unicodedata\n",
    "#def remove_accented_chars(text):\n",
    "#    return unicodedata.normalize('NFKD', text).encode('ascii', 'ignore').decode('utf-8', 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./corpus/e990519_mod.htm\", \"r\", encoding=\"utf-8\") as f:\n",
    "    original_data = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_soup = bs4(original_data, features=\"html.parser\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "html_only_text = html_soup.get_text().lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text = nltk.WordPunctTokenizer().tokenize(html_only_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(set(tokenized_text)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab before Alphabetical filter: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [ x.lower() for x in vocab if x.isalpha() ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab after Alphabetical filter: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stword = stopwords.words(\"spanish\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [ x for x in vocab if x not in stword ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab without stopwords: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:100]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Lematizacion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp_nlp = spacy.load(\"es_core_news_lg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_vocab = set()\n",
    "for v in vocab:\n",
    "    sp_doc = sp_nlp(v)\n",
    "    word_lemma = [ t for t in sp_doc ][0].lemma_\n",
    "    #print(v, word_lemma.split(\" \")[0])\n",
    "    lemmatized_vocab.add(word_lemma.split(\" \")[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(lemmatized_vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = sorted(list(lemmatized_vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size after Lemmatization [no duplicates]: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2nd time remove stop words\n",
    "vocab = [ x for x in vocab if x not in stword ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Vocab size final: {}\".format(len(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab[:100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab.txt\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.writelines( [ \"{}\\n\".format(x) for x in vocab ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminar tokens no alfabeticas\n",
    "clean_text = [ x for x in tokenized_text if x.isalpha() ]\n",
    "# Eliminar stopwords del texto principal\n",
    "clean_text = [ x.lower() for x in clean_text if x.lower() not in stword ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lematizar el texto original\n",
    "lemmatized_text = []\n",
    "for item in clean_text:\n",
    "    sp_doc = sp_nlp(item)\n",
    "    word_lemma = [ t for t in sp_doc ][0].lemma_\n",
    "    #print(v, word_lemma.split(\" \")[0])\n",
    "    lemmatized_text.append(word_lemma.split(\" \")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmatized_text = [ x for x in lemmatized_text if x not in stword ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vsm_data = {}\n",
    "temp_words_array = np.array(lemmatized_text)\n",
    "for word in tqdm(vocab):\n",
    "    indices = np.where(temp_words_array == word)[0]\n",
    "    word_vocab_coincidences = [0] * len(vocab)\n",
    "    word_contexts = set()\n",
    "    for idx in indices:\n",
    "        context = \" \".join(temp_words_array[idx-4:idx+5])\n",
    "        word_contexts.add(context)\n",
    "    #print(\"Word: {} - Context: {}\".format(word, word_contexts))\n",
    "    for cntx in word_contexts:\n",
    "        for w in cntx.split():\n",
    "            if w == word:\n",
    "                continue\n",
    "            vocab_idx = vocab.index(w)\n",
    "            #print(\"Cntx Word: {} - Vocab Index: {}\".format(w, vocab_idx))\n",
    "            word_vocab_coincidences[vocab_idx] = word_vocab_coincidences[vocab_idx] + 1\n",
    "    \n",
    "    vsm_data[word] = list(word_vocab_coincidences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vsm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"vocab_embeddings.json\", \"w\", encoding=\"utf8\") as f:\n",
    "    json.dump(vsm_data, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vsm_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_items_to_export = 1000\n",
    "matrix = [ [] ] * total_items_to_export\n",
    "for i in range(0, total_items_to_export):\n",
    "    matrix[i] = [0] * total_items_to_export\n",
    "for i in tqdm(range(0, len(vocab[:total_items_to_export]))):\n",
    "    actual_item = vsm_data[vocab[i]]\n",
    "    for j in range(i, len(vocab[:total_items_to_export])):\n",
    "        to_compare = vsm_data[vocab[j]]\n",
    "        cosine_simil = np.dot(actual_item,to_compare)/(np.linalg.norm(actual_item)*np.linalg.norm(to_compare))\n",
    "        matrix[i][j] = round(cosine_simil, 5)\n",
    "        matrix[j][i] = round(cosine_simil, 5)\n",
    "        #print(i,j, cosine_simil, matrix[i][j])\n",
    "    #print(matrix)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"simil_table.csv\", \"w\", encoding=\"utf8\") as f:\n",
    "    f.write( \",\".join( [\"\"] + vocab[:total_items_to_export]) )\n",
    "    f.write(\"\\n\")\n",
    "    for i in range(0, total_items_to_export):\n",
    "        f.write(\",\".join([vocab[i]] + [str(x) for x in matrix[i]]))\n",
    "        f.write(\"\\n\")\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
